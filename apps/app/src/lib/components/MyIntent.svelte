<script lang="ts">
	import { fade } from 'svelte/transition';
	import { createEventDispatcher } from 'svelte';
	import AudioVisualizer from './AudioVisualizer.svelte';
	import { onMount, onDestroy } from 'svelte';
	import { conversationManager, conversationStore } from '$lib/stores/intentStore';
	import dayjs from 'dayjs';
	import relativeTime from 'dayjs/plugin/relativeTime';
	import MessageItem from './MessageItem.svelte';
	import { hominioAgent } from '$lib/agents/hominioAgent';
	import { goto } from '$app/navigation';
	import { dynamicView } from '$lib/stores';
	import { createMutation } from '$lib/wundergraph';

	// Initialize dayjs relative time plugin
	dayjs.extend(relativeTime);

	export let isOpen = false;
	export let session: any;

	const dispatch = createEventDispatcher();
	let currentAction: { action: string; view: any } | null = null;
	let modalState:
		| 'idle'
		| 'recording'
		| 'processing'
		| 'result'
		| 'need-permissions'
		| 'permissions-denied'
		| 'pioneer-list' = 'idle';
	let currentConversation: any;
	let audioStream: MediaStream | null = null;
	let mediaRecorder: MediaRecorder | null = null;
	let audioChunks: Blob[] = [];
	let messageContainer: HTMLDivElement;

	let hominioAudio: HTMLAudioElement | null = null;
	let visualizerMode: 'user' | 'hominio' = 'user';

	// Type definitions
	interface Message {
		id: string;
		// Add other message properties as needed
	}

	interface Conversation {
		id: string;
		messages: Message[];
	}

	// Safe store subscription with proper typing
	conversationStore.subscribe((state) => {
		if (!state) {
			currentConversation = null;
			return;
		}

		const conversations = state.conversations || [];
		const currentConversationId = state.currentConversationId;

		if (currentConversationId && Array.isArray(conversations)) {
			currentConversation = conversations.find((conv) => conv.id === currentConversationId) || null;
		} else {
			currentConversation = null;
		}
	});

	onMount(() => {
		if (isOpen) {
			conversationManager.startNewConversation();
		}
	});

	onDestroy(() => {
		if (audioStream) {
			audioStream.getTracks().forEach((track) => track.stop());
			audioStream = null;
		}
	});

	function handleClose() {
		resetConversationState();
		dispatch('close');
	}

	type ModalState =
		| 'idle'
		| 'recording'
		| 'processing'
		| 'result'
		| 'need-permissions'
		| 'permissions-denied'
		| 'pioneer-list';

	let hasPermissions = false;
	let permissionRequesting = false;

	onMount(async () => {
		try {
			const result = await navigator.permissions.query({ name: 'microphone' as PermissionName });
			if (result.state === 'granted') {
				hasPermissions = true;
				modalState = 'idle'; // Set to idle to show default message view
			} else if (result.state === 'denied') {
				modalState = 'permissions-denied';
			} else {
				modalState = 'need-permissions';
			}
		} catch (error) {
			console.error('[Error] Permission check failed:', error);
			modalState = 'need-permissions';
		}
	});

	async function requestMicrophonePermissions() {
		if (hasPermissions) return true;
		if (permissionRequesting) return false;

		try {
			permissionRequesting = true;
			const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
			stream.getTracks().forEach((track) => track.stop());
			hasPermissions = true;
			modalState = 'idle'; // Set to idle first
			return true;
		} catch (error) {
			console.error('[Error] Microphone permission denied:', error);
			modalState = 'permissions-denied';
			return false;
		} finally {
			permissionRequesting = false;
		}
	}

	// Create the mutation
	const transcribeAudioMutation = createMutation({
		operationName: 'transcribeAudio'
	});

	async function setupRecording() {
		try {
			const stream = await navigator.mediaDevices.getUserMedia({ audio: true });

			// Force WebM format for all platforms
			const options = {
				mimeType: 'audio/webm;codecs=opus',
				audioBitsPerSecond: 128000
			};

			mediaRecorder = new MediaRecorder(stream, options);

			mediaRecorder.ondataavailable = (event) => {
				if (event.data.size > 0) {
					audioChunks.push(event.data);
				}
			};

			return true;
		} catch (err) {
			console.error('Recording setup failed:', err);
			return false;
		}
	}

	async function handleRecordingComplete() {
		if (!mediaRecorder) return;

		try {
			// Create blob
			const audioBlob = new Blob(audioChunks, { type: 'audio/webm' });

			// Convert to base64
			const base64 = await new Promise<string>((resolve) => {
				const reader = new FileReader();
				reader.onloadend = () => resolve(reader.result as string);
				reader.readAsDataURL(audioBlob);
			});

			// Send to API
			const response = await $transcribeAudioMutation.mutateAsync({
				audioBase64: base64
			});

			// Handle response
			if (response.data.text) {
				// Success handling
			} else {
				// Error handling
			}
		} catch (error) {
			console.error('Processing failed:', error);
		} finally {
			// Cleanup
			audioChunks = [];
			mediaRecorder = null;
		}
	}

	export async function handleLongPressStart() {
		try {
			console.log('🎤 Starting recording setup...');
			audioStream = await navigator.mediaDevices.getUserMedia({ audio: true });

			// Log available MIME types
			const types = ['audio/webm;codecs=opus', 'audio/mp4', 'audio/ogg;codecs=opus', 'audio/webm'];
			types.forEach((type) => {
				console.log(`🎤 MIME type ${type} supported:`, MediaRecorder.isTypeSupported(type));
			});

			const options = {
				mimeType: 'audio/webm;codecs=opus',
				audioBitsPerSecond: 128000
			};
			console.log('🎙️ Initializing recorder with options:', options);

			mediaRecorder = new MediaRecorder(audioStream, options);
			audioChunks = [];

			mediaRecorder.ondataavailable = (event) => {
				if (event.data.size > 0) {
					console.log('📊 Received audio chunk:', event.data.size, 'bytes');
					audioChunks.push(event.data);
				}
			};

			mediaRecorder.start();
			console.log('🎤 Recording started with MIME type:', mediaRecorder.mimeType);
			modalState = 'recording';
		} catch (error) {
			console.error('❌ Recording setup failed:', error);
			modalState = 'idle';
		}
	}

	export async function handleLongPressEnd() {
		if (!mediaRecorder) return;

		try {
			modalState = 'working';
			console.log('🎤 Starting transcription process...');
			console.log('📊 Current recorder state:', {
				state: mediaRecorder.state,
				mimeType: mediaRecorder.mimeType,
				chunksCount: audioChunks.length
			});

			mediaRecorder.stop();

			await new Promise<void>((resolve) => {
				mediaRecorder!.onstop = () => resolve();
			});
			console.log('🛑 Media recorder stopped');

			const audioBlob = new Blob(audioChunks, { type: 'audio/webm' });
			console.log('📦 Audio blob details:', {
				size: audioBlob.size,
				type: audioBlob.type,
				mimeType: audioBlob.type,
				chunksLength: audioChunks.length
			});

			const reader = new FileReader();
			const base64 = await new Promise<string>((resolve) => {
				reader.onloadend = () => {
					const result = reader.result as string;
					console.log('📝 Base64 conversion details:', {
						resultLength: result.length,
						startsWith: result.substring(0, 50) + '...',
						containsHeader: result.includes('data:audio/')
					});
					resolve(result);
				};
				reader.readAsDataURL(audioBlob);
			});

			console.log('🚀 Sending to API...');
			const response = await $transcribeAudioMutation.mutateAsync({
				audioBase64: base64
			});
			console.log('✅ API Response:', response);

			if (response.data.text) {
				console.log('📝 Transcription successful:', response.data.text);
				handleTranscriptionComplete(response.data.text);
			} else if (response.data.error === 'pioneer-list') {
				console.log('ℹ️ Pioneer list response received');
				modalState = 'pioneer-list';
			} else {
				throw new Error(response.data.error);
			}
		} catch (error) {
			console.error('❌ Processing failed:', error);
			console.error('Error details:', {
				name: error instanceof Error ? error.name : 'Unknown',
				message: error instanceof Error ? error.message : 'Unknown error',
				stack: error instanceof Error ? error.stack : undefined
			});
			modalState = 'error';
		} finally {
			if (modalState !== 'working') {
				console.log('🧹 Cleaning up audio resources');
				if (audioStream) {
					audioStream.getTracks().forEach((track) => {
						track.stop();
						console.log('🎤 Audio track stopped:', track.kind);
					});
				}
				audioChunks = [];
				mediaRecorder = null;
				audioStream = null;
			}
		}
	}

	function handleActionComplete() {
		currentAction = null;
	}

	// Add this helper function at the top with other functions
	function getRandomWorkingAudio() {
		const audioNum = Math.floor(Math.random() * 5) + 1;
		return `/audio/workingonit${audioNum}.mp3`;
	}

	async function playHominioResponse() {
		visualizerMode = 'hominio';
		hominioAudio = new Audio(getRandomWorkingAudio());

		hominioAudio.play();
		return new Promise((resolve) => {
			hominioAudio!.addEventListener('ended', () => {
				resolve(true);
			});
		});
	}

	// Modify handleTranscriptionComplete to not start a new audio playback if one is already playing
	async function handleTranscriptionComplete(text: string) {
		try {
			visualizerMode = 'hominio';

			// Only start new audio playback if none is currently playing
			const audioPromise = hominioAudio?.paused ? playHominioResponse() : Promise.resolve(true);

			const [response] = await Promise.all([
				hominioAgent.processRequest(text, conversationManager.getMessageContext() || []),
				audioPromise
			]);

			modalState = 'result';
			visualizerMode = 'user';

			// Handle different payload types
			if (response?.message?.payload) {
				switch (response.message.payload.type) {
					case 'view':
						handleViewPayload(response.message.payload);
						break;
					case 'form':
						handleFormPayload(response.message.payload);
						break;
					case 'action':
						handleActionPayload(response.message.payload);
						break;
					case 'data':
						handleDataPayload(response.message.payload);
						break;
				}
			}

			await new Promise((resolve) => setTimeout(resolve, 100));
			scrollToBottom();
		} catch (error) {
			console.error('Error processing request:', error);
			modalState = 'error';
			conversationManager.addMessage(
				'Sorry, I encountered an error processing your request.',
				'system',
				'error'
			);
		} finally {
			// Cleanup audio
			if (hominioAudio) {
				hominioAudio.pause();
				hominioAudio = null;
			}
		}
	}

	async function handleViewPayload(payload: any) {
		if (payload?.data?.view) {
			try {
				// Update the view
				dynamicView.set({ view: payload.data.view });

				// Wait a brief moment to ensure view update is processed
				await new Promise((resolve) => setTimeout(resolve, 100));

				// Close the modal and reset state
				if (payload.data.success) {
					resetConversationState();
					dispatch('close');

					// Navigate if needed
					if (window.location.pathname !== '/me') {
						await goto('/me');
					}
				}
			} catch (error) {
				console.error('Error handling view payload:', error);
			}
		}
	}

	function handleFormPayload(payload: FormPayload) {
		// Handle form rendering logic
	}

	function handleActionPayload(payload: ActionPayload) {
		// Handle action execution logic
	}

	function handleDataPayload(payload: DataPayload) {
		// Handle data display logic
	}

	// Update the scrollToBottom function to be more robust
	function scrollToBottom() {
		if (messageContainer) {
			// Use requestAnimationFrame to ensure DOM is updated
			requestAnimationFrame(() => {
				// Smooth scroll to bottom
				messageContainer.scrollTo({
					top: messageContainer.scrollHeight,
					behavior: 'smooth'
				});
			});
		}
	}

	// Add scroll handling when messages update
	$: if (currentConversation?.messages?.length) {
		scrollToBottom();
	}

	// Add function to reset conversation state
	function resetConversationState() {
		// Reset audio state
		if (audioStream) {
			audioStream.getTracks().forEach((track) => track.stop());
		}
		audioStream = null;
		mediaRecorder = null;
		audioChunks = [];

		// Reset modal state
		modalState = 'idle';

		// Reset conversation
		conversationManager.endCurrentConversation();
	}

	// Clean up on component destroy
	onDestroy(() => {
		resetConversationState();
	});

	// Add this helper function near your other utility functions
	function getSupportedMimeType() {
		// Order matters - try most compatible formats first
		const types = [
			'audio/webm;codecs=opus', // Chrome/Firefox
			'audio/mp4', // Safari/iOS
			'audio/ogg;codecs=opus', // Fallback
			'audio/webm' // Last resort
		];

		for (const type of types) {
			if (MediaRecorder.isTypeSupported(type)) {
				return type;
			}
		}

		throw new Error('No supported audio format found');
	}
</script>

{#if isOpen}
	<div
		class="fixed inset-0 z-40 bg-surface-800/50 {!currentAction?.type?.includes('ai')
			? 'backdrop-blur-sm'
			: ''}"
		class:hidden={!isOpen}
		on:click|self={() => dispatch('close')}
		transition:fade={{ duration: 200 }}
	>
		<div class="container h-full max-w-2xl mx-auto" on:click|stopPropagation>
			<div class="fixed inset-x-0 bottom-0 z-50 flex justify-center p-4 mb-16">
				<div
					class="w-full max-w-6xl overflow-hidden border shadow-xl rounded-xl bg-surface-700 border-surface-600"
				>
					<!-- Header -->
					<div class="flex items-center justify-between p-4 border-b border-surface-600">
						<h2 class="text-lg font-semibold text-tertiary-200">
							{#if modalState === 'recording'}
								Recording...
							{:else if modalState === 'processing'}
								Processing...
							{:else if modalState === 'need-permissions'}
								Microphone Access
							{:else if modalState === 'permissions-denied'}
								Access Denied
							{:else if modalState === 'pioneer-list'}
								Authentication Error
							{:else}
								Your Message
							{/if}
						</h2>
						<button
							class="flex items-center justify-center w-8 h-8 transition-colors rounded-full hover:bg-surface-600/50"
							on:click={handleClose}
						>
							<svg
								class="w-6 h-6 text-tertiary-200"
								xmlns="http://www.w3.org/2000/svg"
								fill="none"
								viewBox="0 0 24 24"
								stroke="currentColor"
							>
								<path
									stroke-linecap="round"
									stroke-linejoin="round"
									stroke-width="2"
									d="M6 18L18 6M6 6l12 12"
								/>
							</svg>
						</button>
					</div>

					<!-- Content -->
					<div class="flex flex-col min-h-[200px] max-h-[60vh] overflow-y-auto p-6">
						{#if modalState === 'need-permissions'}
							<div class="flex flex-col items-center justify-center flex-1 space-y-4 text-center">
								<div class="text-4xl">🎤</div>
								<h3 class="text-xl font-semibold text-tertiary-200">Microphone Access Needed</h3>
								<p class="text-surface-200">
									Please allow microphone access to start making voice requests.
								</p>
							</div>
						{:else if modalState === 'permissions-denied'}
							<div class="flex flex-col items-center justify-center flex-1 space-y-4 text-center">
								<div class="text-4xl">🚫</div>
								<h3 class="text-xl font-semibold text-tertiary-200">Microphone Access Denied</h3>
								<p class="text-surface-200">
									Please enable microphone access in your browser settings.
								</p>
								<div class="text-sm text-surface-300">
									<p>How to enable:</p>
									<ol class="mt-2 text-left list-decimal list-inside">
										<li>Click the lock icon in your browser's address bar</li>
										<li>Find "Microphone" in the permissions list</li>
										<li>Change the setting to "Allow"</li>
										<li>Refresh the page</li>
									</ol>
								</div>
							</div>
						{:else if modalState === 'recording'}
							<div class="flex items-center justify-center flex-1">
								<AudioVisualizer
									isRecording={true}
									{audioStream}
									mode={visualizerMode}
									audioElement={null}
								/>
							</div>
						{:else if modalState === 'processing'}
							<div class="flex items-center justify-center flex-1">
								<AudioVisualizer
									isRecording={false}
									audioStream={null}
									mode={visualizerMode}
									audioElement={hominioAudio}
								/>
							</div>
						{:else if modalState === 'result'}
							<div
								class="flex-1 space-y-4 overflow-y-auto scroll-smooth"
								bind:this={messageContainer}
								style="scroll-behavior: smooth;"
							>
								{#if currentConversation?.messages?.length}
									{#each currentConversation.messages as message (message.id)}
										<MessageItem {message} {session} on:actionComplete={handleActionComplete} />
									{/each}
								{:else}
									<div class="flex items-center justify-center flex-1">
										<p class="text-lg text-tertiary-200">Press and hold to record your message</p>
									</div>
								{/if}
							</div>
						{:else if modalState === 'pioneer-list'}
							<div class="flex flex-col items-center justify-center flex-1 space-y-4 text-center">
								<div class="text-4xl">🚀</div>
								<h3 class="text-xl font-semibold text-tertiary-200">Coming Soon!</h3>
								<p class="text-surface-200">Voice commands are currently in beta testing.</p>
								<p class="text-surface-300">
									Get early access by moving up the Visioncreator list! The higher your rank, the
									sooner you'll be invited.
								</p>
							</div>
						{:else}
							<div class="flex items-center justify-center flex-1">
								<p class="text-lg text-tertiary-200">Press and hold to record your message</p>
							</div>
						{/if}
					</div>
				</div>
			</div>
		</div>
	</div>
{/if}

<style>
	/* Add some styles for better interaction feedback */
	[role='button']:focus-visible {
		outline: 2px solid rgb(var(--color-tertiary-400));
		outline-offset: -2px;
	}

	.modal-container {
		@apply bg-surface-800 dark:bg-surface-900 rounded-lg shadow-xl;
	}
</style>
